{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4708788",
   "metadata": {},
   "source": [
    "# Fine-Tuning Language Models for Text Classification: A Deep Practical Guide\n",
    "\n",
    "Welcome to this comprehensive guide on fine-tuning large language models (LLMs) for text classification tasks! \n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "This notebook provides a complete, hands-on approach to:\n",
    "\n",
    "1. **Understanding the Why**: Learn why fine-tuning pre-trained LLMs is crucial for text classification\n",
    "2. **Complete Implementation**: Step-by-step code for the entire fine-tuning pipeline\n",
    "3. **Thai Language Focus**: Special considerations for Thai text processing\n",
    "4. **Best Practices**: Production-ready techniques and optimization strategies\n",
    "5. **Practical Examples**: Real-world applications with sentiment analysis and topic classification\n",
    "\n",
    "## üîß Key Features\n",
    "\n",
    "- **Model Selection**: Choose the right pre-trained model for your task\n",
    "- **Data Preparation**: Handle Thai text preprocessing and tokenization\n",
    "- **Training Pipeline**: Complete fine-tuning with Hugging Face Transformers\n",
    "- **Evaluation Metrics**: Comprehensive model assessment\n",
    "- **Hyperparameter Optimization**: Systematic tuning with Optuna\n",
    "- **Deployment Ready**: Save and load models for production use\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "- Basic Python programming knowledge\n",
    "- Understanding of machine learning concepts\n",
    "- Familiarity with PyTorch (helpful but not required)\n",
    "- GPU access recommended for faster training\n",
    "\n",
    "Let's start building your text classification expertise! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bf397e",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries and Set Up Environment\n",
    "\n",
    "First, let's import all the necessary libraries and set up our environment for reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af257862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core ML and deep learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "\n",
    "# Hugging Face Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "# Dataset handling\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    f1_score, \n",
    "    precision_recall_fscore_support, \n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Thai language processing\n",
    "try:\n",
    "    from pythainlp import word_tokenize\n",
    "    from attacut import tokenize as attacut_tokenize\n",
    "    print(\"‚úÖ Thai NLP libraries loaded successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Thai NLP libraries not found. Install with: pip install pythainlp attacut\")\n",
    "\n",
    "# Hyperparameter optimization (optional)\n",
    "try:\n",
    "    import optuna\n",
    "    print(\"‚úÖ Optuna loaded for hyperparameter optimization\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Optuna not found. Install with: pip install optuna\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"Set random seeds for reproducible results.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "    # For deterministic behavior (may reduce performance)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Check device availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"   Note: GPU not available. Training will be slower on CPU.\")\n",
    "\n",
    "# Configure visualization settings\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"\\nüöÄ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88de471c",
   "metadata": {},
   "source": [
    "## 2. Select and Load a Pre-trained Model\n",
    "\n",
    "Choosing the right pre-trained model is crucial for success. For Thai text classification, we'll use WangchanBERTa, a BERT-based model specifically trained on Thai text.\n",
    "\n",
    "### Model Selection Criteria:\n",
    "\n",
    "1. **Language Support**: Thai-specific models perform better than multilingual ones\n",
    "2. **Task Compatibility**: Models designed for sequence classification\n",
    "3. **Resource Requirements**: Balance between performance and computational cost\n",
    "4. **Domain Relevance**: Models trained on similar domains when available\n",
    "\n",
    "### Popular Thai Language Models:\n",
    "\n",
    "- **WangchanBERTa**: Thai BERT variant (recommended for Thai text)\n",
    "- **multilingual-BERT**: Supports Thai but less specialized\n",
    "- **XLM-RoBERTa**: Cross-lingual model with Thai support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a678f6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
    "MAX_LENGTH = 512\n",
    "NUM_LABELS = 2  # Will be updated based on actual dataset\n",
    "\n",
    "print(f\"üìã Model Configuration:\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Max Length: {MAX_LENGTH}\")\n",
    "print(f\"   Initial Num Labels: {NUM_LABELS}\")\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"\\nüî§ Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Add padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"   Added padding token\")\n",
    "\n",
    "print(f\"‚úÖ Tokenizer loaded successfully\")\n",
    "print(f\"   Vocabulary size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"   Special tokens: {len(tokenizer.special_tokens_map)}\")\n",
    "\n",
    "# Test tokenization with Thai text\n",
    "sample_thai_text = \"‡∏ú‡∏°‡∏ä‡∏≠‡∏ö‡∏†‡∏≤‡∏û‡∏¢‡∏ô‡∏ï‡∏£‡πå‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡∏Å ‡∏™‡∏ô‡∏∏‡∏Å‡πÅ‡∏•‡∏∞‡∏ô‡πà‡∏≤‡∏ï‡∏∑‡πà‡∏ô‡πÄ‡∏ï‡πâ‡∏ô\"\n",
    "sample_english_text = \"I love this movie, it's exciting and entertaining\"\n",
    "\n",
    "print(f\"\\nüîç Tokenization Example:\")\n",
    "print(f\"Thai text: {sample_thai_text}\")\n",
    "\n",
    "# Tokenize and show results\n",
    "thai_tokens = tokenizer.tokenize(sample_thai_text)\n",
    "thai_encoded = tokenizer.encode(sample_thai_text, add_special_tokens=True)\n",
    "\n",
    "print(f\"Tokens: {thai_tokens}\")\n",
    "print(f\"Token IDs: {thai_encoded}\")\n",
    "print(f\"Decoded: {tokenizer.decode(thai_encoded)}\")\n",
    "\n",
    "# Function to load model (we'll do this after preparing the dataset to know exact num_labels)\n",
    "def load_model(num_labels: int):\n",
    "    \"\"\"Load pre-trained model for sequence classification.\"\"\"\n",
    "    print(f\"\\nü§ñ Loading model with {num_labels} labels...\")\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=num_labels,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "    )\n",
    "    \n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"‚úÖ Model loaded successfully\")\n",
    "    print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"   Trainable: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"\\nüìù Note: Model will be loaded after dataset preparation to determine the correct number of labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74074543",
   "metadata": {},
   "source": [
    "## 3. Prepare and Explore the Dataset\n",
    "\n",
    "Data quality is the foundation of successful fine-tuning. Let's create sample datasets and explore their characteristics.\n",
    "\n",
    "### Key Dataset Considerations:\n",
    "\n",
    "1. **Quality over Quantity**: Clean, relevant data is more valuable than large, noisy datasets\n",
    "2. **Class Balance**: Check for imbalanced classes and plan mitigation strategies\n",
    "3. **Representative Samples**: Ensure training data represents real-world usage\n",
    "4. **Proper Splits**: Maintain consistent distributions across train/validation/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40915227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_datasets():\n",
    "    \"\"\"Create sample Thai text classification datasets for demonstration.\"\"\"\n",
    "    \n",
    "    # Thai Sentiment Analysis Dataset\n",
    "    sentiment_data = {\n",
    "        'text': [\n",
    "            '‡∏ú‡∏°‡∏ä‡∏≠‡∏ö‡∏†‡∏≤‡∏û‡∏¢‡∏ô‡∏ï‡∏£‡πå‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡∏Å ‡∏™‡∏ô‡∏∏‡∏Å‡πÅ‡∏•‡∏∞‡∏ô‡πà‡∏≤‡∏ï‡∏∑‡πà‡∏ô‡πÄ‡∏ï‡πâ‡∏ô',\n",
    "            '‡∏£‡πâ‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ô‡∏µ‡πâ‡πÅ‡∏¢‡πà‡∏°‡∏≤‡∏Å ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏≠‡∏£‡πà‡∏≠‡∏¢ ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏î‡∏µ',\n",
    "            '‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏î‡∏µ‡∏°‡∏≤‡∏Å ‡∏£‡∏≤‡∏Ñ‡∏≤‡πÑ‡∏°‡πà‡πÅ‡∏û‡∏á ‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡πà‡∏≤‡πÄ‡∏á‡∏¥‡∏ô ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏•‡∏¢',\n",
    "            '‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡πà‡∏°‡∏≤‡∏Å ‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£ ‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏≤‡∏Å‡∏°‡∏≤‡∏≠‡∏µ‡∏Å',\n",
    "            '‡πÇ‡∏£‡∏á‡πÅ‡∏£‡∏°‡∏ô‡∏µ‡πâ‡∏™‡∏∞‡∏≠‡∏≤‡∏î ‡∏™‡∏∞‡∏î‡∏ß‡∏Å‡∏™‡∏ö‡∏≤‡∏¢ ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏î‡∏µ ‡∏ß‡∏¥‡∏ß‡∏™‡∏ß‡∏¢',\n",
    "            '‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠‡πÄ‡∏•‡πà‡∏°‡∏ô‡∏µ‡πâ‡∏ô‡πà‡∏≤‡πÄ‡∏ö‡∏∑‡πà‡∏≠‡∏°‡∏≤‡∏Å ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏°‡πà‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à ‡πÑ‡∏°‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥',\n",
    "            '‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏≠‡∏£‡πà‡∏≠‡∏¢‡∏°‡∏≤‡∏Å ‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏Å‡∏≤‡∏®‡∏î‡∏µ ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ñ‡∏π‡∏Å ‡∏à‡∏∞‡∏°‡∏≤‡∏≠‡∏µ‡∏Å',\n",
    "            '‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤ ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏ï‡πà‡∏≥ ‡πÄ‡∏™‡∏µ‡∏¢‡πÄ‡∏á‡∏¥‡∏ô',\n",
    "            '‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏î‡∏µ‡∏°‡∏≤‡∏Å ‡πÄ‡∏à‡πâ‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏™‡πà‡πÉ‡∏à‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤ ‡∏û‡∏≠‡πÉ‡∏à‡∏°‡∏≤‡∏Å',\n",
    "            '‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡πà‡∏≤‡πÅ‡∏•‡πâ‡∏ß ‡πÑ‡∏°‡πà‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡πà‡∏≤ ‡∏ú‡∏¥‡∏î‡∏´‡∏ß‡∏±‡∏á‡∏°‡∏≤‡∏Å',\n",
    "            '‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏° ‡∏™‡πà‡∏á‡πÄ‡∏£‡πá‡∏ß ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏î‡∏µ ‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡πÉ‡∏à‡∏°‡∏≤‡∏Å',\n",
    "            '‡πÑ‡∏°‡πà‡∏î‡∏µ‡πÄ‡∏•‡∏¢ ‡πÅ‡∏¢‡πà‡∏°‡∏≤‡∏Å ‡πÑ‡∏°‡πà‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡πà‡∏≤‡πÄ‡∏á‡∏¥‡∏ô ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ã‡∏∑‡πâ‡∏≠‡∏≠‡∏µ‡∏Å',\n",
    "            '‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏° ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢ ‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡πà‡∏≤‡∏°‡∏≤‡∏Å ‡∏ä‡∏≠‡∏ö‡∏°‡∏≤‡∏Å',\n",
    "            '‡πÑ‡∏°‡πà‡∏û‡∏≠‡πÉ‡∏à ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ ‡∏à‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤',\n",
    "            '‡∏¢‡∏≠‡∏î‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏° ‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡∏µ‡∏°‡∏≤‡∏Å ‡∏à‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô',\n",
    "            '‡πÅ‡∏¢‡πà‡∏°‡∏≤‡∏Å ‡∏ú‡∏¥‡∏î‡∏´‡∏ß‡∏±‡∏á ‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡∏ã‡∏∑‡πâ‡∏≠ ‡πÄ‡∏™‡∏µ‡∏¢‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏õ‡∏•‡πà‡∏≤',\n",
    "            '‡∏ô‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏°‡∏≤‡∏Å ‡∏î‡∏µ‡πÑ‡∏ã‡∏ô‡πå‡∏™‡∏ß‡∏¢ ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏î‡∏µ ‡∏£‡∏≤‡∏Ñ‡∏≤‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°',\n",
    "            '‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á ‡πÑ‡∏°‡πà‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠ ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡πà',\n",
    "            '‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡πÉ‡∏à‡∏°‡∏≤‡∏Å ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏î‡∏µ‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏° ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏≠‡∏µ‡∏Å',\n",
    "            '‡πÑ‡∏°‡πà‡∏î‡∏µ ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏ï‡πà‡∏≥ ‡πÑ‡∏°‡πà‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡πà‡∏≤ ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ã‡∏∑‡πâ‡∏≠‡∏≠‡∏µ‡∏Å'\n",
    "        ],\n",
    "        'label': [\n",
    "            'positive', 'negative', 'positive', 'negative', 'positive',\n",
    "            'negative', 'positive', 'negative', 'positive', 'negative',\n",
    "            'positive', 'negative', 'positive', 'negative', 'positive',\n",
    "            'negative', 'positive', 'negative', 'positive', 'negative'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Thai Topic Classification Dataset  \n",
    "    topic_data = {\n",
    "        'text': [\n",
    "            '‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏£‡πà‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡πÇ‡∏Ñ‡∏ß‡∏¥‡∏î-19 ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡πÇ‡∏•‡∏Å ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏´‡∏•‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏†‡∏≤‡∏ß‡∏∞‡∏ñ‡∏î‡∏ñ‡∏≠‡∏¢',\n",
    "            '‡∏ô‡∏±‡∏Å‡∏ü‡∏∏‡∏ï‡∏ö‡∏≠‡∏•‡∏ó‡∏µ‡∏°‡∏ä‡∏≤‡∏ï‡∏¥‡πÑ‡∏ó‡∏¢‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏Ç‡πà‡∏á‡∏Ç‡∏±‡∏ô‡∏ü‡∏∏‡∏ï‡∏ö‡∏≠‡∏•‡πÇ‡∏•‡∏Å‡∏£‡∏≠‡∏ö‡∏Ñ‡∏±‡∏î‡πÄ‡∏•‡∏∑‡∏≠‡∏Å',\n",
    "            '‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ AI ‡πÅ‡∏•‡∏∞ Machine Learning ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏°',\n",
    "            '‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏ï‡πà‡∏≠‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏ô‡∏Å‡∏≤‡∏£‡∏ú‡∏•‡∏¥‡∏ï‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏Ç‡∏ô‡∏™‡πà‡∏á',\n",
    "            '‡∏Å‡∏≤‡∏£‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ü‡∏∑‡πâ‡∏ô‡∏ï‡∏±‡∏ß‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡πÇ‡∏Ñ‡∏ß‡∏¥‡∏î-19 ‡∏Ñ‡∏•‡∏µ‡πà‡∏Ñ‡∏•‡∏≤‡∏¢',\n",
    "            '‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô‡∏°‡∏∑‡∏≠‡∏ñ‡∏∑‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏≠‡∏µ‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏¥‡∏£‡πå‡∏ã‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏¥‡∏¢‡∏°‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô',\n",
    "            '‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏™‡∏≤‡∏ò‡∏≤‡∏£‡∏ì‡∏™‡∏∏‡∏Ç‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏£‡πà‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡πÇ‡∏£‡∏Ñ‡∏ï‡∏¥‡∏î‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï',\n",
    "            '‡∏Å‡∏≤‡∏£‡πÅ‡∏Ç‡πà‡∏á‡∏Ç‡∏±‡∏ô‡∏Å‡∏µ‡∏¨‡∏≤‡πÇ‡∏≠‡∏•‡∏¥‡∏°‡∏õ‡∏¥‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏ó‡∏µ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏±‡∏Å‡∏Å‡∏µ‡∏¨‡∏≤‡∏à‡∏≤‡∏Å‡∏ó‡∏±‡πà‡∏ß‡πÇ‡∏•‡∏Å',\n",
    "            '‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°‡πÉ‡∏ô‡∏î‡πâ‡∏≤‡∏ô Blockchain ‡πÅ‡∏•‡∏∞ Cryptocurrency ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à',\n",
    "            '‡∏†‡∏≤‡∏ß‡∏∞‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏ü‡πâ‡∏≠‡∏™‡πà‡∏á‡∏ú‡∏•‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢ ‡∏ö‡∏£‡∏¥‡πÇ‡∏†‡∏Ñ ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô',\n",
    "            '‡πÄ‡∏ó‡∏®‡∏Å‡∏≤‡∏•‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡πÅ‡∏•‡∏∞‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ó‡πâ‡∏≠‡∏á‡∏ñ‡∏¥‡πà‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡∏Å‡∏£‡∏∞‡∏ï‡∏∏‡πâ‡∏ô‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡∏ä‡∏∏‡∏°‡∏ä‡∏ô',\n",
    "            '‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Big Data ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡πÇ‡∏†‡∏Ñ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏ó‡∏≤‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à'\n",
    "        ],\n",
    "        'label': [\n",
    "            'health', 'sports', 'technology', 'economy', 'travel', 'technology',\n",
    "            'health', 'sports', 'technology', 'economy', 'travel', 'technology'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'sentiment': pd.DataFrame(sentiment_data),\n",
    "        'topic': pd.DataFrame(topic_data)\n",
    "    }\n",
    "\n",
    "# Create sample datasets\n",
    "datasets = create_sample_datasets()\n",
    "\n",
    "print(\"üìä Sample Datasets Created:\")\n",
    "print(\"\\n1. Sentiment Analysis Dataset:\")\n",
    "print(f\"   Total samples: {len(datasets['sentiment'])}\")\n",
    "print(f\"   Classes: {datasets['sentiment']['label'].unique()}\")\n",
    "print(f\"   Class distribution:\")\n",
    "sentiment_counts = datasets['sentiment']['label'].value_counts()\n",
    "for label, count in sentiment_counts.items():\n",
    "    print(f\"     {label}: {count} ({count/len(datasets['sentiment']):.1%})\")\n",
    "\n",
    "print(\"\\n2. Topic Classification Dataset:\")\n",
    "print(f\"   Total samples: {len(datasets['topic'])}\")\n",
    "print(f\"   Classes: {datasets['topic']['label'].unique()}\")\n",
    "print(f\"   Class distribution:\")\n",
    "topic_counts = datasets['topic']['label'].value_counts()\n",
    "for label, count in topic_counts.items():\n",
    "    print(f\"     {label}: {count} ({count/len(datasets['topic']):.1%})\")\n",
    "\n",
    "# Let's use sentiment analysis for our main example\n",
    "df = datasets['sentiment'].copy()\n",
    "print(f\"\\nüéØ Using sentiment analysis dataset for fine-tuning example\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nüìã Sample Data:\")\n",
    "for i in range(3):\n",
    "    print(f\"{i+1}. Text: {df.iloc[i]['text']}\")\n",
    "    print(f\"   Label: {df.iloc[i]['label']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d26dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Sentiment analysis distribution\n",
    "sentiment_counts.plot(kind='bar', ax=ax1, color=['skyblue', 'lightcoral'])\n",
    "ax1.set_title('Sentiment Analysis - Class Distribution')\n",
    "ax1.set_xlabel('Sentiment')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Topic classification distribution\n",
    "topic_counts.plot(kind='bar', ax=ax2, color=['lightgreen', 'orange', 'purple', 'yellow'])\n",
    "ax2.set_title('Topic Classification - Class Distribution')\n",
    "ax2.set_xlabel('Topic')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Text length analysis\n",
    "df['text_length'] = df['text'].str.len()\n",
    "df['word_count'] = df['text'].str.split().str.len()\n",
    "\n",
    "print(\"üìè Text Statistics:\")\n",
    "print(f\"Average text length: {df['text_length'].mean():.1f} characters\")\n",
    "print(f\"Average word count: {df['word_count'].mean():.1f} words\")\n",
    "print(f\"Max text length: {df['text_length'].max()} characters\")\n",
    "print(f\"Min text length: {df['text_length'].min()} characters\")\n",
    "\n",
    "# Plot text length distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df['text_length'], bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Text Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Text Lengths')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df['word_count'], bins=15, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Word Counts')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6bdf91",
   "metadata": {},
   "source": [
    "## 4. Preprocess and Tokenize Data\n",
    "\n",
    "Proper preprocessing and tokenization are crucial for model performance. We'll handle Thai-specific text processing, create label mappings, and prepare data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b449ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_thai_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess Thai text for better model performance.\n",
    "    \n",
    "    Args:\n",
    "        text: Input Thai text\n",
    "        \n",
    "    Returns:\n",
    "        Preprocessed text\n",
    "    \"\"\"\n",
    "    # Remove excessive whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Optional: Apply Thai word segmentation (uncomment if needed)\n",
    "    # text = ' '.join(word_tokenize(text, engine='attacut'))\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Create label mappings\n",
    "unique_labels = df['label'].unique()\n",
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "print(f\"üè∑Ô∏è Label Mappings:\")\n",
    "print(f\"   Labels: {unique_labels}\")\n",
    "print(f\"   label2id: {label2id}\")\n",
    "print(f\"   id2label: {id2label}\")\n",
    "\n",
    "# Update NUM_LABELS\n",
    "NUM_LABELS = len(unique_labels)\n",
    "print(f\"   Number of labels: {NUM_LABELS}\")\n",
    "\n",
    "# Preprocess text and create numeric labels\n",
    "df['text_processed'] = df['text'].apply(preprocess_thai_text)\n",
    "df['labels'] = df['label'].map(label2id)\n",
    "\n",
    "print(f\"\\n‚úÖ Data preprocessing completed\")\n",
    "print(f\"   Original text column: 'text'\")\n",
    "print(f\"   Processed text column: 'text_processed'\")\n",
    "print(f\"   Numeric labels column: 'labels'\")\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "print(f\"\\nüîÄ Splitting data...\")\n",
    "\n",
    "# First split: train+val vs test (80-20)\n",
    "train_val_df, test_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=df['labels']\n",
    ")\n",
    "\n",
    "# Second split: train vs val (75-25 of train_val, resulting in 60-20-20 overall)\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df, \n",
    "    test_size=0.25, \n",
    "    random_state=42, \n",
    "    stratify=train_val_df['labels']\n",
    ")\n",
    "\n",
    "print(f\"   Train set: {len(train_df)} samples ({len(train_df)/len(df):.1%})\")\n",
    "print(f\"   Validation set: {len(val_df)} samples ({len(val_df)/len(df):.1%})\")\n",
    "print(f\"   Test set: {len(test_df)} samples ({len(test_df)/len(df):.1%})\")\n",
    "\n",
    "# Verify class distribution in splits\n",
    "print(f\"\\nüìä Class distribution in splits:\")\n",
    "for split_name, split_df in [('Train', train_df), ('Validation', val_df), ('Test', test_df)]:\n",
    "    dist = split_df['labels'].value_counts().sort_index()\n",
    "    dist_pct = (dist / len(split_df) * 100).round(1)\n",
    "    print(f\"   {split_name}: {dict(zip([id2label[i] for i in dist.index], dist_pct))}\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_texts(texts, max_length=MAX_LENGTH):\n",
    "    \"\"\"Tokenize a list of texts.\"\"\"\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Test tokenization\n",
    "print(f\"\\nüî§ Testing tokenization:\")\n",
    "sample_text = df['text_processed'].iloc[0]\n",
    "sample_encoding = tokenizer(\n",
    "    sample_text,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=MAX_LENGTH,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(f\"   Sample text: {sample_text}\")\n",
    "print(f\"   Input IDs shape: {sample_encoding['input_ids'].shape}\")\n",
    "print(f\"   Attention mask shape: {sample_encoding['attention_mask'].shape}\")\n",
    "print(f\"   Token count: {sample_encoding['input_ids'].shape[1]}\")\n",
    "\n",
    "# Show tokenized version\n",
    "tokens = tokenizer.convert_ids_to_tokens(sample_encoding['input_ids'][0])\n",
    "print(f\"   First 10 tokens: {tokens[:10]}\")\n",
    "print(f\"   Last 10 tokens: {tokens[-10:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81ff78f",
   "metadata": {},
   "source": [
    "## 5. Create Data Loaders\n",
    "\n",
    "Now we'll convert our preprocessed data into Hugging Face Dataset objects and create efficient data loaders for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c329db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset objects from DataFrames\n",
    "def create_dataset(df, text_col='text_processed', label_col='labels'):\n",
    "    \"\"\"Create a Hugging Face Dataset from DataFrame.\"\"\"\n",
    "    return Dataset.from_dict({\n",
    "        'text': df[text_col].tolist(),\n",
    "        'labels': df[label_col].tolist()\n",
    "    })\n",
    "\n",
    "# Create datasets for each split\n",
    "train_dataset = create_dataset(train_df)\n",
    "val_dataset = create_dataset(val_df)\n",
    "test_dataset = create_dataset(test_df)\n",
    "\n",
    "print(f\"üì¶ Created Dataset objects:\")\n",
    "print(f\"   Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"   Validation dataset: {len(val_dataset)} samples\")\n",
    "print(f\"   Test dataset: {len(test_dataset)} samples\")\n",
    "\n",
    "# Tokenization function for datasets\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Function to tokenize examples in a dataset.\"\"\"\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Apply tokenization to all datasets\n",
    "print(f\"\\n‚öôÔ∏è Tokenizing datasets...\")\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(f\"‚úÖ Tokenization completed\")\n",
    "\n",
    "# Create DatasetDict for convenient handling\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "print(f\"\\nüìä Dataset summary:\")\n",
    "print(dataset_dict)\n",
    "\n",
    "# Inspect a sample from the training dataset\n",
    "print(f\"\\nüîç Sample from training dataset:\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"   Text: {sample['text'][:100]}...\")\n",
    "print(f\"   Label: {sample['labels']} ({id2label[sample['labels']]})\")\n",
    "print(f\"   Input IDs shape: {len(sample['input_ids'])}\")\n",
    "print(f\"   Attention mask shape: {len(sample['attention_mask'])}\")\n",
    "\n",
    "# Create data collator for dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "print(f\"\\nüéØ Data collator created for dynamic padding\")\n",
    "print(f\"   This will handle variable-length sequences efficiently during training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8801c7",
   "metadata": {},
   "source": [
    "## 6. Configure Training Hyperparameters\n",
    "\n",
    "Hyperparameter selection significantly impacts training success. Let's set up a comprehensive configuration with explanations for each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eb1951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "training_config = {\n",
    "    # Core training parameters\n",
    "    'learning_rate': 2e-5,          # Common starting point for BERT models\n",
    "    'batch_size': 8,                # Adjust based on GPU memory (16 or 32 if possible)\n",
    "    'num_epochs': 3,                # Usually 2-5 epochs for fine-tuning\n",
    "    'warmup_ratio': 0.1,            # 10% of training steps for warmup\n",
    "    'weight_decay': 0.01,           # L2 regularization\n",
    "    \n",
    "    # Training stability\n",
    "    'max_grad_norm': 1.0,           # Gradient clipping\n",
    "    'gradient_accumulation_steps': 2, # Simulate larger batch size\n",
    "    \n",
    "    # Evaluation and saving\n",
    "    'evaluation_strategy': 'steps',  # Evaluate during training\n",
    "    'eval_steps': 50,               # Steps between evaluations\n",
    "    'save_steps': 100,              # Steps between checkpoints\n",
    "    'logging_steps': 10,            # Steps between logging\n",
    "    \n",
    "    # Model selection\n",
    "    'load_best_model_at_end': True,\n",
    "    'metric_for_best_model': 'f1',\n",
    "    'greater_is_better': True,\n",
    "    \n",
    "    # Performance optimization\n",
    "    'fp16': torch.cuda.is_available(),  # Mixed precision training\n",
    "    'dataloader_num_workers': 0,       # Number of data loading workers\n",
    "    'disable_tqdm': False,             # Show progress bars\n",
    "    \n",
    "    # Early stopping\n",
    "    'early_stopping_patience': 3,      # Stop if no improvement for 3 evaluations\n",
    "    \n",
    "    # Output configuration\n",
    "    'output_dir': './results',\n",
    "    'logging_dir': './logs',\n",
    "    'report_to': None,                 # Disable wandb/tensorboard for now\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è Training Configuration:\")\n",
    "for key, value in training_config.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Calculate effective batch size\n",
    "effective_batch_size = training_config['batch_size'] * training_config['gradient_accumulation_steps']\n",
    "print(f\"\\nüìä Effective batch size: {effective_batch_size}\")\n",
    "\n",
    "# Estimate training steps\n",
    "steps_per_epoch = len(train_dataset) // effective_batch_size\n",
    "total_steps = steps_per_epoch * training_config['num_epochs']\n",
    "warmup_steps = int(total_steps * training_config['warmup_ratio'])\n",
    "\n",
    "print(f\"   Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"   Total training steps: {total_steps}\")\n",
    "print(f\"   Warmup steps: {warmup_steps}\")\n",
    "\n",
    "# Define compute metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute evaluation metrics.\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Calculate various metrics\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1_macro = f1_score(labels, predictions, average='macro')\n",
    "    f1_weighted = f1_score(labels, predictions, average='weighted')\n",
    "    \n",
    "    precision, recall, f1_micro, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='micro'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1_weighted,  # Primary metric for model selection\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_micro': f1_micro,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "print(f\"\\nüìà Evaluation metrics configured:\")\n",
    "print(\"   - Accuracy: Overall correctness\")\n",
    "print(\"   - F1-weighted: Handles class imbalance\")\n",
    "print(\"   - F1-macro: Equal weight to all classes\")\n",
    "print(\"   - F1-micro: Overall micro-averaged performance\")\n",
    "print(\"   - Precision & Recall: Additional detailed metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b668054d",
   "metadata": {},
   "source": [
    "## 7. Fine-Tune the Model\n",
    "\n",
    "Now we'll load the model with the correct number of labels and start the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fcd5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with correct number of labels\n",
    "print(f\"ü§ñ Loading model for {NUM_LABELS} classes...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "print(f\"‚úÖ Model loaded successfully\")\n",
    "print(f\"   Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Create training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=training_config['output_dir'],\n",
    "    learning_rate=training_config['learning_rate'],\n",
    "    per_device_train_batch_size=training_config['batch_size'],\n",
    "    per_device_eval_batch_size=training_config['batch_size'],\n",
    "    num_train_epochs=training_config['num_epochs'],\n",
    "    warmup_ratio=training_config['warmup_ratio'],\n",
    "    weight_decay=training_config['weight_decay'],\n",
    "    max_grad_norm=training_config['max_grad_norm'],\n",
    "    gradient_accumulation_steps=training_config['gradient_accumulation_steps'],\n",
    "    evaluation_strategy=training_config['evaluation_strategy'],\n",
    "    eval_steps=training_config['eval_steps'],\n",
    "    save_steps=training_config['save_steps'],\n",
    "    logging_steps=training_config['logging_steps'],\n",
    "    logging_dir=training_config['logging_dir'],\n",
    "    load_best_model_at_end=training_config['load_best_model_at_end'],\n",
    "    metric_for_best_model=training_config['metric_for_best_model'],\n",
    "    greater_is_better=training_config['greater_is_better'],\n",
    "    fp16=training_config['fp16'],\n",
    "    dataloader_num_workers=training_config['dataloader_num_workers'],\n",
    "    disable_tqdm=training_config['disable_tqdm'],\n",
    "    report_to=training_config['report_to'],\n",
    "    save_strategy='steps',\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=training_config['early_stopping_patience']\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "print(f\"\\nüéØ Trainer configured successfully\")\n",
    "print(f\"   Training dataset: {len(train_dataset)} samples\")\n",
    "print(f\"   Validation dataset: {len(val_dataset)} samples\")\n",
    "print(f\"   Early stopping patience: {training_config['early_stopping_patience']} evaluations\")\n",
    "\n",
    "# Start training\n",
    "print(f\"\\nüöÄ Starting fine-tuning...\")\n",
    "print(f\"   This may take several minutes depending on your hardware\")\n",
    "print(f\"   Monitor the progress below:\")\n",
    "\n",
    "# Train the model\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(f\"\\n‚úÖ Training completed!\")\n",
    "print(f\"   Training time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"   Training samples per second: {train_result.metrics['train_samples_per_second']:.2f}\")\n",
    "print(f\"   Final training loss: {train_result.metrics['train_loss']:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(training_config['output_dir'])\n",
    "\n",
    "print(f\"\\nüíæ Model saved to: {training_config['output_dir']}\")\n",
    "print(\"   Files saved:\")\n",
    "print(\"   - pytorch_model.bin (model weights)\")\n",
    "print(\"   - config.json (model configuration)\")\n",
    "print(\"   - tokenizer files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f08005",
   "metadata": {},
   "source": [
    "## 8. Evaluate Model Performance\n",
    "\n",
    "Let's comprehensively evaluate our fine-tuned model using various metrics and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61615af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"üìä Evaluating on test set...\")\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "print(f\"üéØ Test Set Results:\")\n",
    "for metric, value in test_results.items():\n",
    "    if metric.startswith('eval_'):\n",
    "        metric_name = metric.replace('eval_', '')\n",
    "        print(f\"   {metric_name}: {value:.4f}\")\n",
    "\n",
    "# Get predictions for detailed analysis\n",
    "print(f\"\\nüîç Getting predictions for detailed analysis...\")\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "y_true = predictions.label_ids\n",
    "\n",
    "# Generate classification report\n",
    "print(f\"\\nüìã Classification Report:\")\n",
    "report = classification_report(\n",
    "    y_true, y_pred, \n",
    "    target_names=list(id2label.values()),\n",
    "    digits=4\n",
    ")\n",
    "print(report)\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues',\n",
    "    xticklabels=list(id2label.values()),\n",
    "    yticklabels=list(id2label.values())\n",
    ")\n",
    "plt.title('Confusion Matrix - Test Set')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-class metrics\n",
    "print(f\"\\nüìà Per-Class Performance:\")\n",
    "for i, class_name in id2label.items():\n",
    "    class_mask = (y_true == i)\n",
    "    if class_mask.sum() > 0:\n",
    "        class_acc = (y_pred[class_mask] == y_true[class_mask]).mean()\n",
    "        print(f\"   {class_name}: {class_acc:.4f} accuracy ({class_mask.sum()} samples)\")\n",
    "\n",
    "# Analyze misclassifications\n",
    "print(f\"\\nüîç Misclassification Analysis:\")\n",
    "misclassified = (y_pred != y_true)\n",
    "misclassified_indices = np.where(misclassified)[0]\n",
    "\n",
    "print(f\"   Total misclassifications: {misclassified.sum()}\")\n",
    "print(f\"   Misclassification rate: {misclassified.mean():.4f}\")\n",
    "\n",
    "if len(misclassified_indices) > 0:\n",
    "    print(f\"\\n   Sample misclassifications:\")\n",
    "    for i, idx in enumerate(misclassified_indices[:3]):  # Show first 3\n",
    "        true_label = id2label[y_true[idx]]\n",
    "        pred_label = id2label[y_pred[idx]]\n",
    "        text = test_dataset[idx]['text']\n",
    "        print(f\"   {i+1}. Text: {text[:100]}...\")\n",
    "        print(f\"      True: {true_label} | Predicted: {pred_label}\")\n",
    "        print()\n",
    "\n",
    "# Get prediction probabilities for confidence analysis\n",
    "prediction_probs = torch.softmax(torch.tensor(predictions.predictions), dim=1)\n",
    "max_probs = prediction_probs.max(dim=1).values\n",
    "confidence_scores = max_probs.numpy()\n",
    "\n",
    "print(f\"üìä Prediction Confidence Analysis:\")\n",
    "print(f\"   Mean confidence: {confidence_scores.mean():.4f}\")\n",
    "print(f\"   Median confidence: {np.median(confidence_scores):.4f}\")\n",
    "print(f\"   Min confidence: {confidence_scores.min():.4f}\")\n",
    "print(f\"   Max confidence: {confidence_scores.max():.4f}\")\n",
    "\n",
    "# Plot confidence distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(confidence_scores, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Confidence Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Prediction Confidence')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Confidence vs accuracy\n",
    "correct_predictions = (y_pred == y_true)\n",
    "plt.scatter(confidence_scores[correct_predictions], [1]*correct_predictions.sum(), \n",
    "           alpha=0.6, color='green', label='Correct', s=10)\n",
    "plt.scatter(confidence_scores[~correct_predictions], [0]*misclassified.sum(), \n",
    "           alpha=0.6, color='red', label='Incorrect', s=10)\n",
    "plt.xlabel('Confidence Score')\n",
    "plt.ylabel('Correctness')\n",
    "plt.title('Confidence vs Correctness')\n",
    "plt.legend()\n",
    "plt.yticks([0, 1], ['Incorrect', 'Correct'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create summary metrics dictionary\n",
    "final_metrics = {\n",
    "    'test_accuracy': test_results['eval_accuracy'],\n",
    "    'test_f1_weighted': test_results['eval_f1'],\n",
    "    'test_f1_macro': test_results['eval_f1_macro'],\n",
    "    'test_precision': test_results['eval_precision'],\n",
    "    'test_recall': test_results['eval_recall'],\n",
    "    'misclassification_rate': misclassified.mean(),\n",
    "    'mean_confidence': confidence_scores.mean(),\n",
    "    'total_samples': len(y_true)\n",
    "}\n",
    "\n",
    "print(f\"\\nüìä Final Performance Summary:\")\n",
    "for metric, value in final_metrics.items():\n",
    "    print(f\"   {metric}: {value:.4f}\")\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame({\n",
    "    'text': [test_dataset[i]['text'] for i in range(len(test_dataset))],\n",
    "    'true_label': [id2label[label] for label in y_true],\n",
    "    'predicted_label': [id2label[label] for label in y_pred],\n",
    "    'confidence': confidence_scores,\n",
    "    'correct': y_pred == y_true\n",
    "})\n",
    "\n",
    "results_df.to_csv('test_results.csv', index=False)\n",
    "print(f\"\\nüíæ Detailed results saved to 'test_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabffba9",
   "metadata": {},
   "source": [
    "## 9. Optimize and Tune Hyperparameters\n",
    "\n",
    "Now let's explore hyperparameter optimization to potentially improve our model's performance using Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78d15b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter search space\n",
    "def create_hyperparameter_search_space():\n",
    "    \"\"\"Define the search space for hyperparameter optimization.\"\"\"\n",
    "    return {\n",
    "        'learning_rate': (1e-6, 1e-4),     # Learning rate range\n",
    "        'batch_size': [8, 16],             # Batch size options\n",
    "        'num_epochs': (2, 4),              # Number of epochs\n",
    "        'warmup_ratio': (0.0, 0.2),       # Warmup ratio\n",
    "        'weight_decay': (0.0, 0.3),       # Weight decay\n",
    "    }\n",
    "\n",
    "# Hyperparameter optimization function\n",
    "def hyperparameter_optimization(n_trials=10):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter optimization using Optuna.\n",
    "    \n",
    "    Args:\n",
    "        n_trials: Number of optimization trials\n",
    "    \n",
    "    Returns:\n",
    "        Best hyperparameters and study object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import optuna\n",
    "    except ImportError:\n",
    "        print(\"‚ùå Optuna not installed. Skipping hyperparameter optimization.\")\n",
    "        print(\"   Install with: pip install optuna\")\n",
    "        return None, None\n",
    "    \n",
    "    def objective(trial):\n",
    "        \"\"\"Objective function for Optuna optimization.\"\"\"\n",
    "        \n",
    "        # Sample hyperparameters\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-6, 1e-4, log=True)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [8, 16])\n",
    "        num_epochs = trial.suggest_int('num_epochs', 2, 4)\n",
    "        warmup_ratio = trial.suggest_float('warmup_ratio', 0.0, 0.2)\n",
    "        weight_decay = trial.suggest_float('weight_decay', 0.0, 0.3)\n",
    "        \n",
    "        # Create model for this trial\n",
    "        trial_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            num_labels=NUM_LABELS,\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "        )\n",
    "        trial_model.to(device)\n",
    "        \n",
    "        # Configure training arguments\n",
    "        trial_args = TrainingArguments(\n",
    "            output_dir=f'./hp_search_trial_{trial.number}',\n",
    "            learning_rate=learning_rate,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=num_epochs,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            weight_decay=weight_decay,\n",
    "            evaluation_strategy='epoch',\n",
    "            save_strategy='epoch',\n",
    "            logging_steps=50,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model='f1',\n",
    "            greater_is_better=True,\n",
    "            report_to=None,\n",
    "            disable_tqdm=True,  # Disable progress bars for cleaner output\n",
    "            fp16=torch.cuda.is_available(),\n",
    "        )\n",
    "        \n",
    "        # Create trainer\n",
    "        trial_trainer = Trainer(\n",
    "            model=trial_model,\n",
    "            args=trial_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "        \n",
    "        # Train and evaluate\n",
    "        trial_trainer.train()\n",
    "        eval_results = trial_trainer.evaluate()\n",
    "        \n",
    "        # Clean up\n",
    "        del trial_model\n",
    "        del trial_trainer\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return eval_results['eval_f1']\n",
    "    \n",
    "    # Create study\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    return study.best_params, study\n",
    "\n",
    "# Run hyperparameter optimization (optional - can be time-consuming)\n",
    "run_hp_optimization = False  # Set to True to run optimization\n",
    "\n",
    "if run_hp_optimization:\n",
    "    print(\"üîß Starting hyperparameter optimization...\")\n",
    "    print(\"   This may take significant time depending on n_trials\")\n",
    "    \n",
    "    best_params, study = hyperparameter_optimization(n_trials=5)  # Reduced for demo\n",
    "    \n",
    "    if best_params:\n",
    "        print(f\"\\n‚úÖ Hyperparameter optimization completed!\")\n",
    "        print(f\"   Best parameters: {best_params}\")\n",
    "        print(f\"   Best F1 score: {study.best_value:.4f}\")\n",
    "        \n",
    "        # Visualize optimization results\n",
    "        if hasattr(optuna.visualization, 'plot_optimization_history'):\n",
    "            fig = optuna.visualization.plot_optimization_history(study)\n",
    "            fig.show()\n",
    "        \n",
    "        print(f\"\\nüí° Recommendations:\")\n",
    "        print(f\"   Use these parameters for your final model training\")\n",
    "        print(f\"   Consider running more trials for better results\")\n",
    "    \n",
    "else:\n",
    "    print(\"üîß Hyperparameter optimization skipped\")\n",
    "    print(\"   Set run_hp_optimization = True to run optimization\")\n",
    "    print(\"   Recommended search space:\")\n",
    "    search_space = create_hyperparameter_search_space()\n",
    "    for param, space in search_space.items():\n",
    "        print(f\"     {param}: {space}\")\n",
    "\n",
    "# Manual hyperparameter experimentation\n",
    "print(f\"\\nüß™ Manual Hyperparameter Experimentation:\")\n",
    "print(\"   Here are some common hyperparameter combinations to try:\")\n",
    "\n",
    "hp_combinations = [\n",
    "    {'learning_rate': 2e-5, 'batch_size': 16, 'num_epochs': 3, 'warmup_ratio': 0.1},\n",
    "    {'learning_rate': 3e-5, 'batch_size': 8, 'num_epochs': 4, 'warmup_ratio': 0.06},\n",
    "    {'learning_rate': 1e-5, 'batch_size': 32, 'num_epochs': 2, 'warmup_ratio': 0.15},\n",
    "    {'learning_rate': 5e-5, 'batch_size': 16, 'num_epochs': 3, 'warmup_ratio': 0.0},\n",
    "]\n",
    "\n",
    "for i, combo in enumerate(hp_combinations, 1):\n",
    "    print(f\"   Combination {i}: {combo}\")\n",
    "\n",
    "print(f\"\\nüí° Hyperparameter Tuning Tips:\")\n",
    "print(\"   1. Start with learning rates between 1e-5 and 5e-5\")\n",
    "print(\"   2. Use smaller batch sizes if you have limited GPU memory\")\n",
    "print(\"   3. Try 2-4 epochs to avoid overfitting\")\n",
    "print(\"   4. Use warmup for training stability\")\n",
    "print(\"   5. Apply weight decay (0.01-0.1) for regularization\")\n",
    "print(\"   6. Monitor validation metrics to detect overfitting\")\n",
    "\n",
    "# Regularization techniques\n",
    "print(f\"\\nüõ°Ô∏è Additional Regularization Techniques:\")\n",
    "print(\"   - Dropout: Already included in the pre-trained model\")\n",
    "print(\"   - Weight decay: Configurable in training arguments\")\n",
    "print(\"   - Early stopping: Implemented in our training\")\n",
    "print(\"   - Data augmentation: Consider for small datasets\")\n",
    "print(\"   - Learning rate scheduling: Can be added to training arguments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a950b2c",
   "metadata": {},
   "source": [
    "## 10. Save and Deploy the Fine-Tuned Model\n",
    "\n",
    "Let's save our model properly and demonstrate how to load it for inference in production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6f5383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "model_save_path = \"./final_model\"\n",
    "print(f\"üíæ Saving final model to: {model_save_path}\")\n",
    "\n",
    "# Save model and tokenizer\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "# Save additional metadata\n",
    "import json\n",
    "\n",
    "model_metadata = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'num_labels': NUM_LABELS,\n",
    "    'label2id': label2id,\n",
    "    'id2label': {str(k): v for k, v in id2label.items()},  # JSON requires string keys\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'training_config': training_config,\n",
    "    'final_metrics': final_metrics,\n",
    "    'classes': list(label2id.keys())\n",
    "}\n",
    "\n",
    "with open(f\"{model_save_path}/model_metadata.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(model_metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úÖ Model saved successfully!\")\n",
    "print(\"   Files saved:\")\n",
    "print(\"   - pytorch_model.bin (model weights)\")\n",
    "print(\"   - config.json (model configuration)\")\n",
    "print(\"   - tokenizer.json (tokenizer)\")\n",
    "print(\"   - model_metadata.json (custom metadata)\")\n",
    "\n",
    "# Demonstrate loading the saved model\n",
    "print(f\"\\nüîÑ Demonstrating model loading...\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(model_save_path)\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(model_save_path)\n",
    "loaded_model.to(device)\n",
    "\n",
    "# Load metadata\n",
    "with open(f\"{model_save_path}/model_metadata.json\", 'r', encoding='utf-8') as f:\n",
    "    loaded_metadata = json.load(f)\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"   Model: {loaded_metadata['model_name']}\")\n",
    "print(f\"   Classes: {loaded_metadata['classes']}\")\n",
    "\n",
    "# Create inference pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create text classification pipeline\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=loaded_model,\n",
    "    tokenizer=loaded_tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "print(f\"\\nüöÄ Inference pipeline created!\")\n",
    "\n",
    "# Test inference with new examples\n",
    "test_texts = [\n",
    "    \"‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ô‡∏µ‡πâ‡∏î‡∏µ‡∏°‡∏≤‡∏Å ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏° ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏•‡∏¢\",  # Should be positive\n",
    "    \"‡πÑ‡∏°‡πà‡∏û‡∏≠‡πÉ‡∏à ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡πà‡∏°‡∏≤‡∏Å ‡πÑ‡∏°‡πà‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡πà‡∏≤\",        # Should be negative\n",
    "    \"‡∏£‡∏≤‡∏Ñ‡∏≤‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏õ‡∏Å‡∏ï‡∏¥\",              # Neutral/positive\n",
    "]\n",
    "\n",
    "print(f\"\\nüîç Testing inference:\")\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    result = classifier(text)\n",
    "    print(f\"{i}. Text: {text}\")\n",
    "    print(f\"   Prediction: {result[0]['label']} (confidence: {result[0]['score']:.4f})\")\n",
    "    print()\n",
    "\n",
    "# Custom inference function for more control\n",
    "def predict_sentiment(text, return_probabilities=False):\n",
    "    \"\"\"\n",
    "    Custom inference function with preprocessing.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to classify\n",
    "        return_probabilities: Whether to return probabilities for all classes\n",
    "        \n",
    "    Returns:\n",
    "        Prediction result\n",
    "    \"\"\"\n",
    "    # Preprocess text\n",
    "    processed_text = preprocess_thai_text(text)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = loaded_tokenizer(\n",
    "        processed_text,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get predictions\n",
    "    loaded_model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        if return_probabilities:\n",
    "            return {\n",
    "                'probabilities': probabilities.cpu().numpy()[0],\n",
    "                'labels': list(loaded_metadata['classes'])\n",
    "            }\n",
    "        else:\n",
    "            predicted_class_id = torch.argmax(probabilities, dim=-1).item()\n",
    "            predicted_label = loaded_metadata['id2label'][str(predicted_class_id)]\n",
    "            confidence = probabilities.max().item()\n",
    "            \n",
    "            return {\n",
    "                'label': predicted_label,\n",
    "                'confidence': confidence\n",
    "            }\n",
    "\n",
    "print(f\"üß™ Testing custom inference function:\")\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    result = predict_sentiment(text)\n",
    "    prob_result = predict_sentiment(text, return_probabilities=True)\n",
    "    \n",
    "    print(f\"{i}. Text: {text}\")\n",
    "    print(f\"   Prediction: {result['label']} (confidence: {result['confidence']:.4f})\")\n",
    "    print(f\"   All probabilities: {dict(zip(prob_result['labels'], prob_result['probabilities'].round(4)))}\")\n",
    "    print()\n",
    "\n",
    "# Create a simple API endpoint example (using FastAPI)\n",
    "api_code = '''\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Load model at startup\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./final_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./final_model\")\n",
    "\n",
    "class TextInput(BaseModel):\n",
    "    text: str\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(input_data: TextInput):\n",
    "    # Tokenize and predict\n",
    "    inputs = tokenizer(input_data.text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probabilities = torch.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "        confidence = probabilities.max().item()\n",
    "    \n",
    "    return {\n",
    "        \"prediction\": model.config.id2label[predicted_class],\n",
    "        \"confidence\": float(confidence)\n",
    "    }\n",
    "\n",
    "# Run with: uvicorn api:app --reload\n",
    "'''\n",
    "\n",
    "print(f\"üì° API Deployment Example:\")\n",
    "print(\"   Save the following code as 'api.py' and run with:\")\n",
    "print(\"   uvicorn api:app --reload\")\n",
    "print(f\"   Then test with: curl -X POST http://localhost:8000/predict -H 'Content-Type: application/json' -d '{{\\\"text\\\": \\\"Your Thai text here\\\"}}'\")\n",
    "\n",
    "# Save API code\n",
    "with open(\"api_example.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(api_code)\n",
    "\n",
    "print(f\"\\nüíæ API example saved as 'api_example.py'\")\n",
    "\n",
    "# Performance optimization tips\n",
    "print(f\"\\n‚ö° Performance Optimization Tips:\")\n",
    "print(\"   1. Model Quantization: Reduce model size and inference time\")\n",
    "print(\"   2. ONNX Conversion: Convert to ONNX for faster inference\")\n",
    "print(\"   3. TensorRT: Use NVIDIA TensorRT for GPU optimization\")\n",
    "print(\"   4. Batch Processing: Process multiple texts together\")\n",
    "print(\"   5. Caching: Cache frequent predictions\")\n",
    "print(\"   6. Model Distillation: Create smaller student models\")\n",
    "\n",
    "print(f\"\\nüéØ Deployment Checklist:\")\n",
    "print(\"   ‚úÖ Model saved with all necessary files\")\n",
    "print(\"   ‚úÖ Inference pipeline tested\")\n",
    "print(\"   ‚úÖ Custom prediction function created\")\n",
    "print(\"   ‚úÖ API example provided\")\n",
    "print(\"   ‚úÖ Performance optimization tips listed\")\n",
    "print(\"\\nüöÄ Your model is ready for production deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eead23e",
   "metadata": {},
   "source": [
    "## 11. Monitor Model and Handle Data Drift\n",
    "\n",
    "Production deployment requires ongoing monitoring to ensure model performance remains stable over time. Let's implement monitoring strategies and data drift detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d288f1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "\n",
    "class ModelMonitor:\n",
    "    \"\"\"\n",
    "    A simple monitoring system for text classification models in production.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"Thai-Sentiment-Classifier\"):\n",
    "        self.model_name = model_name\n",
    "        self.predictions_log = []\n",
    "        self.performance_metrics = defaultdict(list)\n",
    "        self.confidence_thresholds = {\n",
    "            'high': 0.8,    # High confidence predictions\n",
    "            'medium': 0.6,  # Medium confidence predictions\n",
    "            'low': 0.4      # Low confidence predictions (need review)\n",
    "        }\n",
    "        \n",
    "    def log_prediction(self, text: str, prediction: str, confidence: float, timestamp: datetime.datetime = None):\n",
    "        \"\"\"Log a single prediction with metadata.\"\"\"\n",
    "        if timestamp is None:\n",
    "            timestamp = datetime.datetime.now()\n",
    "            \n",
    "        log_entry = {\n",
    "            'timestamp': timestamp,\n",
    "            'text': text,\n",
    "            'prediction': prediction,\n",
    "            'confidence': confidence,\n",
    "            'text_length': len(text),\n",
    "            'word_count': len(text.split())\n",
    "        }\n",
    "        \n",
    "        self.predictions_log.append(log_entry)\n",
    "        \n",
    "    def analyze_confidence_distribution(self, recent_days: int = 7):\n",
    "        \"\"\"Analyze confidence distribution over recent predictions.\"\"\"\n",
    "        cutoff_date = datetime.datetime.now() - datetime.timedelta(days=recent_days)\n",
    "        recent_predictions = [\n",
    "            p for p in self.predictions_log \n",
    "            if p['timestamp'] >= cutoff_date\n",
    "        ]\n",
    "        \n",
    "        if not recent_predictions:\n",
    "            print(f\"‚ö†Ô∏è No predictions found in the last {recent_days} days\")\n",
    "            return\n",
    "            \n",
    "        confidences = [p['confidence'] for p in recent_predictions]\n",
    "        \n",
    "        high_conf = sum(1 for c in confidences if c >= self.confidence_thresholds['high'])\n",
    "        medium_conf = sum(1 for c in confidences if self.confidence_thresholds['medium'] <= c < self.confidence_thresholds['high'])\n",
    "        low_conf = sum(1 for c in confidences if c < self.confidence_thresholds['medium'])\n",
    "        \n",
    "        total = len(confidences)\n",
    "        \n",
    "        print(f\"üìä Confidence Distribution (Last {recent_days} days):\")\n",
    "        print(f\"   Total predictions: {total}\")\n",
    "        print(f\"   High confidence (‚â•{self.confidence_thresholds['high']}): {high_conf} ({high_conf/total:.1%})\")\n",
    "        print(f\"   Medium confidence ({self.confidence_thresholds['medium']}-{self.confidence_thresholds['high']}): {medium_conf} ({medium_conf/total:.1%})\")\n",
    "        print(f\"   Low confidence (<{self.confidence_thresholds['medium']}): {low_conf} ({low_conf/total:.1%})\")\n",
    "        \n",
    "        if low_conf / total > 0.2:  # More than 20% low confidence\n",
    "            print(\"   ‚ö†Ô∏è Warning: High proportion of low-confidence predictions!\")\n",
    "            print(\"   Consider model retraining or additional data collection.\")\n",
    "            \n",
    "        return {\n",
    "            'high': high_conf / total,\n",
    "            'medium': medium_conf / total,\n",
    "            'low': low_conf / total\n",
    "        }\n",
    "    \n",
    "    def detect_input_drift(self, reference_stats: dict = None, recent_days: int = 7):\n",
    "        \"\"\"Detect potential input drift by comparing text characteristics.\"\"\"\n",
    "        cutoff_date = datetime.datetime.now() - datetime.timedelta(days=recent_days)\n",
    "        recent_predictions = [\n",
    "            p for p in self.predictions_log \n",
    "            if p['timestamp'] >= cutoff_date\n",
    "        ]\n",
    "        \n",
    "        if not recent_predictions:\n",
    "            print(f\"‚ö†Ô∏è No recent predictions to analyze\")\n",
    "            return\n",
    "            \n",
    "        # Calculate current statistics\n",
    "        current_stats = {\n",
    "            'avg_text_length': np.mean([p['text_length'] for p in recent_predictions]),\n",
    "            'avg_word_count': np.mean([p['word_count'] for p in recent_predictions]),\n",
    "            'max_text_length': max([p['text_length'] for p in recent_predictions]),\n",
    "            'min_text_length': min([p['text_length'] for p in recent_predictions])\n",
    "        }\n",
    "        \n",
    "        print(f\"üìè Input Characteristics (Last {recent_days} days):\")\n",
    "        for stat, value in current_stats.items():\n",
    "            print(f\"   {stat}: {value:.2f}\")\n",
    "            \n",
    "        # Compare with reference if provided\n",
    "        if reference_stats:\n",
    "            print(f\"\\nüîç Drift Detection:\")\n",
    "            drift_detected = False\n",
    "            \n",
    "            for stat in ['avg_text_length', 'avg_word_count']:\n",
    "                if stat in reference_stats:\n",
    "                    reference_value = reference_stats[stat]\n",
    "                    current_value = current_stats[stat]\n",
    "                    change_pct = abs(current_value - reference_value) / reference_value * 100\n",
    "                    \n",
    "                    print(f\"   {stat}: {current_value:.2f} vs {reference_value:.2f} (Œî{change_pct:.1f}%)\")\n",
    "                    \n",
    "                    if change_pct > 20:  # More than 20% change\n",
    "                        print(f\"     ‚ö†Ô∏è Significant drift detected!\")\n",
    "                        drift_detected = True\n",
    "                        \n",
    "            if drift_detected:\n",
    "                print(f\"\\nüö® Input drift detected! Consider:\")\n",
    "                print(\"   - Reviewing recent data quality\")\n",
    "                print(\"   - Collecting new training data\")\n",
    "                print(\"   - Retraining the model\")\n",
    "        \n",
    "        return current_stats\n",
    "    \n",
    "    def generate_monitoring_report(self):\n",
    "        \"\"\"Generate a comprehensive monitoring report.\"\"\"\n",
    "        if not self.predictions_log:\n",
    "            print(\"üìã No predictions to report\")\n",
    "            return\n",
    "            \n",
    "        # Overall statistics\n",
    "        total_predictions = len(self.predictions_log)\n",
    "        date_range = (min(p['timestamp'] for p in self.predictions_log),\n",
    "                     max(p['timestamp'] for p in self.predictions_log))\n",
    "        \n",
    "        print(f\"üìã Model Monitoring Report - {self.model_name}\")\n",
    "        print(f\"   Report generated: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"   Total predictions: {total_predictions}\")\n",
    "        print(f\"   Date range: {date_range[0].strftime('%Y-%m-%d')} to {date_range[1].strftime('%Y-%m-%d')}\")\n",
    "        \n",
    "        # Confidence analysis\n",
    "        confidences = [p['confidence'] for p in self.predictions_log]\n",
    "        print(f\"\\nüìä Overall Confidence Statistics:\")\n",
    "        print(f\"   Mean: {np.mean(confidences):.4f}\")\n",
    "        print(f\"   Median: {np.median(confidences):.4f}\")\n",
    "        print(f\"   Std: {np.std(confidences):.4f}\")\n",
    "        print(f\"   Min: {np.min(confidences):.4f}\")\n",
    "        print(f\"   Max: {np.max(confidences):.4f}\")\n",
    "        \n",
    "        # Prediction distribution\n",
    "        predictions = [p['prediction'] for p in self.predictions_log]\n",
    "        pred_counts = pd.Series(predictions).value_counts()\n",
    "        print(f\"\\nüìà Prediction Distribution:\")\n",
    "        for pred, count in pred_counts.items():\n",
    "            print(f\"   {pred}: {count} ({count/total_predictions:.1%})\")\n",
    "\n",
    "# Initialize monitoring system\n",
    "monitor = ModelMonitor(\"Thai-Sentiment-Classifier\")\n",
    "\n",
    "print(\"üîç Model Monitoring System Initialized\")\n",
    "\n",
    "# Simulate some predictions for demonstration\n",
    "print(\"\\nüìù Simulating predictions for monitoring demo...\")\n",
    "\n",
    "# Generate sample monitoring data\n",
    "sample_monitoring_data = [\n",
    "    (\"‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏î‡∏µ‡∏°‡∏≤‡∏Å ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°\", \"positive\", 0.95),\n",
    "    (\"‡πÑ‡∏°‡πà‡∏û‡∏≠‡πÉ‡∏à ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡πà\", \"negative\", 0.88),\n",
    "    (\"‡∏£‡∏≤‡∏Ñ‡∏≤‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ\", \"positive\", 0.72),\n",
    "    (\"‡πÑ‡∏°‡πà‡∏î‡∏µ ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏ï‡πà‡∏≥\", \"negative\", 0.83),\n",
    "    (\"‡∏õ‡∏Å‡∏ï‡∏¥ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤\", \"positive\", 0.65),\n",
    "    (\"‡πÅ‡∏¢‡πà‡∏°‡∏≤‡∏Å ‡πÑ‡∏°‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥\", \"negative\", 0.91),\n",
    "    (\"‡πÇ‡∏≠‡πÄ‡∏Ñ ‡∏û‡∏≠‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ\", \"positive\", 0.55),  # Low confidence\n",
    "    (\"‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏î‡∏µ\", \"positive\", 0.52),  # Low confidence\n",
    "]\n",
    "\n",
    "# Log predictions\n",
    "base_time = datetime.datetime.now() - datetime.timedelta(days=5)\n",
    "for i, (text, pred, conf) in enumerate(sample_monitoring_data):\n",
    "    timestamp = base_time + datetime.timedelta(hours=i*3)\n",
    "    monitor.log_prediction(text, pred, conf, timestamp)\n",
    "\n",
    "print(f\"‚úÖ Logged {len(sample_monitoring_data)} sample predictions\")\n",
    "\n",
    "# Analyze confidence distribution\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "confidence_dist = monitor.analyze_confidence_distribution(recent_days=7)\n",
    "\n",
    "# Reference statistics for drift detection (from training data)\n",
    "training_reference_stats = {\n",
    "    'avg_text_length': df['text_length'].mean(),\n",
    "    'avg_word_count': df['word_count'].mean(),\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "current_stats = monitor.detect_input_drift(\n",
    "    reference_stats=training_reference_stats, \n",
    "    recent_days=7\n",
    ")\n",
    "\n",
    "# Generate full monitoring report\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "monitor.generate_monitoring_report()\n",
    "\n",
    "# Create monitoring dashboard visualization\n",
    "def create_monitoring_dashboard(monitor):\n",
    "    \"\"\"Create visualizations for monitoring dashboard.\"\"\"\n",
    "    \n",
    "    if not monitor.predictions_log:\n",
    "        print(\"No data to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Extract data\n",
    "    timestamps = [p['timestamp'] for p in monitor.predictions_log]\n",
    "    confidences = [p['confidence'] for p in monitor.predictions_log]\n",
    "    predictions = [p['prediction'] for p in monitor.predictions_log]\n",
    "    \n",
    "    # Create plots\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Confidence over time\n",
    "    ax1.plot(timestamps, confidences, marker='o', linestyle='-', alpha=0.7)\n",
    "    ax1.axhline(y=monitor.confidence_thresholds['high'], color='g', linestyle='--', label='High threshold')\n",
    "    ax1.axhline(y=monitor.confidence_thresholds['medium'], color='orange', linestyle='--', label='Medium threshold')\n",
    "    ax1.set_title('Prediction Confidence Over Time')\n",
    "    ax1.set_xlabel('Time')\n",
    "    ax1.set_ylabel('Confidence')\n",
    "    ax1.legend()\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Confidence distribution\n",
    "    ax2.hist(confidences, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax2.axvline(x=monitor.confidence_thresholds['high'], color='g', linestyle='--', label='High threshold')\n",
    "    ax2.axvline(x=monitor.confidence_thresholds['medium'], color='orange', linestyle='--', label='Medium threshold')\n",
    "    ax2.set_title('Confidence Distribution')\n",
    "    ax2.set_xlabel('Confidence')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Prediction distribution\n",
    "    pred_counts = pd.Series(predictions).value_counts()\n",
    "    pred_counts.plot(kind='bar', ax=ax3, color=['lightcoral', 'lightblue'])\n",
    "    ax3.set_title('Prediction Distribution')\n",
    "    ax3.set_xlabel('Prediction')\n",
    "    ax3.set_ylabel('Count')\n",
    "    ax3.tick_params(axis='x', rotation=0)\n",
    "    \n",
    "    # Confidence by prediction\n",
    "    df_monitor = pd.DataFrame(monitor.predictions_log)\n",
    "    for pred in df_monitor['prediction'].unique():\n",
    "        subset = df_monitor[df_monitor['prediction'] == pred]\n",
    "        ax4.scatter(subset.index, subset['confidence'], label=pred, alpha=0.7)\n",
    "    ax4.set_title('Confidence by Prediction Type')\n",
    "    ax4.set_xlabel('Prediction Index')\n",
    "    ax4.set_ylabel('Confidence')\n",
    "    ax4.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nüìä Creating monitoring dashboard...\")\n",
    "create_monitoring_dashboard(monitor)\n",
    "\n",
    "# Production monitoring recommendations\n",
    "print(f\"\\nüéØ Production Monitoring Recommendations:\")\n",
    "print(\"1. **Real-time Monitoring**:\")\n",
    "print(\"   - Set up alerts for low confidence predictions\")\n",
    "print(\"   - Monitor prediction distribution shifts\")\n",
    "print(\"   - Track response times and system performance\")\n",
    "\n",
    "print(\"\\n2. **Data Quality Checks**:\")\n",
    "print(\"   - Validate input text format and encoding\")\n",
    "print(\"   - Check for unusual characters or patterns\")\n",
    "print(\"   - Monitor text length distributions\")\n",
    "\n",
    "print(\"\\n3. **Performance Monitoring**:\")\n",
    "print(\"   - Track model accuracy on labeled validation sets\")\n",
    "print(\"   - Monitor user feedback and corrections\")\n",
    "print(\"   - A/B test model versions\")\n",
    "\n",
    "print(\"\\n4. **Automated Actions**:\")\n",
    "print(\"   - Flag low-confidence predictions for human review\")\n",
    "print(\"   - Trigger retraining when drift is detected\")\n",
    "print(\"   - Scale infrastructure based on prediction volume\")\n",
    "\n",
    "print(\"\\n5. **Logging and Storage**:\")\n",
    "print(\"   - Store predictions with metadata\")\n",
    "print(\"   - Log model versions and configurations\")\n",
    "print(\"   - Maintain audit trails for compliance\")\n",
    "\n",
    "# Save monitoring code as a standalone module\n",
    "monitoring_code = '''\n",
    "\"\"\"\n",
    "Production monitoring module for text classification models.\n",
    "\"\"\"\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "class ProductionModelMonitor:\n",
    "    \"\"\"Production-ready model monitoring system.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, log_file: str = \"model_predictions.log\"):\n",
    "        self.model_name = model_name\n",
    "        self.log_file = log_file\n",
    "        self.setup_logging()\n",
    "    \n",
    "    def setup_logging(self):\n",
    "        \"\"\"Set up logging configuration.\"\"\"\n",
    "        logging.basicConfig(\n",
    "            filename=self.log_file,\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def log_prediction(self, \n",
    "                      text: str, \n",
    "                      prediction: str, \n",
    "                      confidence: float,\n",
    "                      user_id: Optional[str] = None,\n",
    "                      session_id: Optional[str] = None):\n",
    "        \"\"\"Log prediction with full metadata.\"\"\"\n",
    "        \n",
    "        log_data = {\n",
    "            'model_name': self.model_name,\n",
    "            'timestamp': datetime.datetime.now().isoformat(),\n",
    "            'text': text,\n",
    "            'prediction': prediction,\n",
    "            'confidence': confidence,\n",
    "            'text_length': len(text),\n",
    "            'word_count': len(text.split()),\n",
    "            'user_id': user_id,\n",
    "            'session_id': session_id\n",
    "        }\n",
    "        \n",
    "        self.logger.info(json.dumps(log_data, ensure_ascii=False))\n",
    "    \n",
    "    def should_flag_for_review(self, confidence: float, threshold: float = 0.6) -> bool:\n",
    "        \"\"\"Determine if prediction should be flagged for human review.\"\"\"\n",
    "        return confidence < threshold\n",
    "\n",
    "# Usage example:\n",
    "# monitor = ProductionModelMonitor(\"thai-sentiment-v1\")\n",
    "# monitor.log_prediction(\"sample text\", \"positive\", 0.85, user_id=\"user123\")\n",
    "'''\n",
    "\n",
    "with open(\"production_monitoring.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(monitoring_code)\n",
    "\n",
    "print(f\"\\nüíæ Production monitoring code saved as 'production_monitoring.py'\")\n",
    "print(\"\\nüéâ Monitoring setup complete! Your model is ready for production deployment with comprehensive monitoring.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e99eb4",
   "metadata": {},
   "source": [
    "## üéâ Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You've completed the comprehensive guide to fine-tuning language models for text classification. \n",
    "\n",
    "### üèÜ What You've Accomplished\n",
    "\n",
    "1. ‚úÖ **Environment Setup**: Configured all necessary libraries and dependencies\n",
    "2. ‚úÖ **Model Selection**: Chose appropriate pre-trained models for Thai text\n",
    "3. ‚úÖ **Data Preparation**: Created, explored, and preprocessed datasets\n",
    "4. ‚úÖ **Model Fine-tuning**: Implemented complete training pipeline\n",
    "5. ‚úÖ **Evaluation**: Comprehensive performance assessment with multiple metrics\n",
    "6. ‚úÖ **Hyperparameter Optimization**: Systematic tuning strategies\n",
    "7. ‚úÖ **Deployment**: Production-ready model saving and inference\n",
    "8. ‚úÖ **Monitoring**: Ongoing performance tracking and drift detection\n",
    "\n",
    "### üöÄ Key Takeaways\n",
    "\n",
    "- **Quality Data Matters**: Clean, representative datasets are crucial for success\n",
    "- **Hyperparameter Tuning**: Systematic optimization significantly improves performance\n",
    "- **Evaluation is Critical**: Use multiple metrics to assess model performance\n",
    "- **Production Readiness**: Proper saving, loading, and monitoring are essential\n",
    "- **Continuous Improvement**: Regular monitoring and retraining maintain performance\n",
    "\n",
    "### üìà Next Steps for Advanced Users\n",
    "\n",
    "1. **Advanced Techniques**:\n",
    "   - Implement multi-task learning\n",
    "   - Explore few-shot learning approaches\n",
    "   - Try ensemble methods for better performance\n",
    "\n",
    "2. **Scale Up**:\n",
    "   - Use larger datasets for better generalization\n",
    "   - Experiment with larger pre-trained models\n",
    "   - Implement distributed training\n",
    "\n",
    "3. **Domain-Specific Improvements**:\n",
    "   - Create domain-specific vocabularies\n",
    "   - Implement custom preprocessing for your use case\n",
    "   - Explore task-specific architectures\n",
    "\n",
    "4. **Production Enhancement**:\n",
    "   - Implement A/B testing frameworks\n",
    "   - Add comprehensive logging and alerting\n",
    "   - Optimize for low-latency inference\n",
    "\n",
    "### üõ†Ô∏è Tools and Resources for Further Learning\n",
    "\n",
    "- **Hugging Face Transformers**: Official documentation and tutorials\n",
    "- **Papers with Code**: Latest research and implementations\n",
    "- **Thai NLP Resources**: PyThaiNLP, AI Research Thailand\n",
    "- **Monitoring Tools**: Weights & Biases, MLflow, TensorBoard\n",
    "\n",
    "### üìö Recommended Reading\n",
    "\n",
    "- \"Natural Language Processing with Transformers\" by Lewis Tunstall\n",
    "- \"Hands-On Machine Learning\" by Aur√©lien G√©ron\n",
    "- Research papers on Thai NLP and text classification\n",
    "\n",
    "### ü§ù Community and Support\n",
    "\n",
    "- Join Thai NLP communities and forums\n",
    "- Contribute to open-source projects\n",
    "- Share your experiences and learn from others\n",
    "\n",
    "Thank you for following this comprehensive guide! Your journey in fine-tuning language models for text classification has just begun. Keep experimenting, learning, and building amazing NLP applications! üéØ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
